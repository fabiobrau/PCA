\documentclass[10pt]{beamer}
\usepackage{bbm}

\usepackage{graphicx}
\renewcommand\appendixname{Appendix}
\usepackage{xcolor}
\usepackage{tikz}
\colorlet{rred}{red!80!black}
\colorlet{ggreen}{green!80!black}




\usetheme[progressbar=foot]{metropolis}
\usepackage{appendixnumberbeamer}
\setbeamercovered{dynamic}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setlength{\abovecaptionskip}{-10pt plus 0pt minus 0pt}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{obs}{Observation}

% math symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\YY}{\mathcal{Y}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\title{An introduction to PCA}
\subtitle{Weekly AI pills}
\date{2020-10-16}
\author{Fabio Brau.}
\institute{SSSA, Emerging Digital Technologies, Pisa.}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\usebackgroundtemplate{%
    \begin{picture}(300,271)
      \hspace{11.2cm}
       \includegraphics[scale=0.1]{pic/logoretis_noname.png}
   \end{picture}}

\begin{document}
{\usebackgroundtemplate{%
    \begin{picture}(300,265)
      \hspace{0.9cm}
       \includegraphics[scale=0.5]{pic/tecip_logo.png}
       \hspace{0.5cm}
       \includegraphics[scale=0.21]{pic/logoretis_320.png}
   \end{picture}}%
\maketitle
}
\begin{frame}{Summary}
  \begin{itemize}
    \item The aim of Principal Component Analysis
    \item Derivation
      \begin{enumerate}
          \item A Geometrical idea
          \item A statistical Derivation
          \item Singolar Value Decomposition
      \end{enumerate}
    \item PCA from Encoder Decoder NN
    \item Dummy examples
  \end{itemize}
\end{frame}
\section{Geometrical Introduction}
\begin{frame}{Geometrical Introduction}
  Let $X \in \R^{N\times n}$ be a dataset of $N$
  {\bf observation} within $n$ {\bf variables}. 
  \begin{equation}
    X =
    \begin{bmatrix}
      \, & x_1 ^ T &\,\\
      \, & \vdots &\,\\
      \, & x_N ^ T &\,\\
    \end{bmatrix}
    =
    \begin{bmatrix}
      & & & \\
      x^{(1)} &\vline& \cdots& \vline& x^{(n)}\\
      & & & \\
    \end{bmatrix}
    \label{Geometrical view}
  \end{equation}
  {\bf Notations:}
  \begin{itemize}
    \item $x_i\in\R^n$ represents a single {\bf observation}, i.e a {\bf
      sample} in the feature space.
    \item $x^{(i)}\in\R^N$ represents the single {\bf variable}, i.e a {\bf
      column} of the dataset.
    \item The object $\1_n\in\R^n$ is the unitary columnar vector of length
      $n$ $\1_n=[1,\cdots,1]$.
  \end{itemize}
\end{frame}
\begin{frame}{Geometrical Introduction: Finding a principal direction.}
  \begin{minipage}{0.5\textwidth}
    \begin{enumerate}
      \item \uncover<1->{Scalar product measures the projection of $x_j$ along
        the direction $w$.}
      \item \uncover<2->{We are only interested on module.}
      \item \uncover<3->{Summation over samples to get the global
        projection's contribute.}
      \item \uncover<4->{Searching for $w$ which maximizes projection.}
      \item \uncover<5->{Adding constraint to avoid $w\to\infty$ solution.}
    \end{enumerate}
  \end{minipage}\hfill
  \begin{minipage}{0.4\textwidth}
      \begin{equation*}
        \uncover<4->{w_1 \in\argmax_{\uncover<5>{\|w\|_2 = 1}}}
        \uncover<3->{\sum_{j=1}^N}
        \uncover<2->{\left(}\uncover<1->{w\cdot x_j}\uncover<2->{\right)^2}
      \end{equation*}
    \begin{figure}[b]
      \centering
      \includegraphics<1-2>[clip, scale=0.3, trim=3.1cm 0 0cm 0cm]{./pic/dummy_2.eps}
      \includegraphics<3->[clip, scale=0.3, trim=3.1cm 0 0cm 0cm]{./pic/dummy_3.eps}
    \end{figure}
  \end{minipage}
\end{frame}
\begin{frame}{Geometrical Introduction: Finding other directions}
  We search for other orthogonal directions which maximize projections.
  \begin{equation*}
  \begin{aligned}
    w_1 \in \argmax_{\|w\|_2=1}\sum_{j=1}^N (w\cdot x)^2 &\\
    w_2 \in \argmax_{\|w\|_2=1}\sum_{j=1}^N (w\cdot x)^2 &\quad\mbox{and}\quad w_2\perp w_1\\
    \vdots \\
    w_n \in \argmax_{\|w\|_2=1}\sum_{j=1}^N (w\cdot x)^2 &\quad\mbox{and}\quad w_2\perp \left\{
    w_1,\ldots,w_{n-1} \right\}
  \end{aligned}
\end{equation*}
\begin{center}
  \href{./pic/PCA_3D.html}{\bf Example}
\end{center}
\end{frame}
\metroset{block=fill}
\begin{frame}{Geometrical Introduction: Direction Selection}
  \[
    V(w)=\sum_j (w \cdot x_j)^2 \qquad\mbox{\bf momentum along $w$}
  \]
  \onslide<2->
  If $w_1,\,w_2,\,w_3$ orthogonal that maximizes $V$ in the 3D example, then
  \begin{enumerate}
    \item $V(w_1) =  3181.20$\hfill\uncover<3->{\approx82.5\%}
    \item $V(w_2) = 646.25$\hfill\uncover<3->{\approx 17.0\%}
    \item $V(w_3) = 19.23$ \hfill\uncover<3->{\approx 0.5 \%}
  \end{enumerate}
    \onslide<4->
    \begin{center}
      {\bf What if we forget the last direction?}
    \end{center}
  \onslide<5->
  \begin{block}{Observation}
    \begin{itemize}
        \item $x_j =\alpha_{1j}w_1+\alpha_{2j}w_2+\alpha_{3j}w_3$
          (where $\alpha_{ij} = w_i\cdot x_j$).
        \item $\tilde x_j = \alpha_{1j}w_1 + \alpha_{2j}w_2$. 
    \end{itemize}
    \begin{equation}
      \frac{1}{N}\sum_j \|x_j - \tilde x_j\|^2 = \frac{V(w_3)}{N}\approx
      4.8\,10^{-3}
      \tag{MSE}
    \end{equation}
  \end{block}
\end{frame}
\begin{frame}{Geometrical Introduction: Conclusion}
  \begin{itemize}
    \item Given a set of data $X\in\R^{N\times n}$
    \item We can find $w_1,\cdots,w_n$ principal (orthonormal) directions the
      maximize their momentum.
    \item $V(w_1) > V(w_2)>\cdots> V(w_n)$
    \item Approximating X with $\tilde X$ by taking only the first $k$ directions we are
    getting an error that is $V(w_{k+1})/N$
  \end{itemize}
  \onslide<2->
  {\bf\centering What's the catch?}
  \onslide<3->
   \begin{equation}
     \begin{aligned}
       \max_{w\in\R^n}\quad & \sum_{j=1}^N (w \cdot x_j)^2\\
       \mbox{s.t}\quad & w_i \cdot w =0, \,\forall i<k \\
       & w\cdot w = 1\\
    \end{aligned}
    \label{MP}
    \tag{MP}
  \end{equation} 
\end{frame}
\section{Statistical Derivation}
\end{document}

