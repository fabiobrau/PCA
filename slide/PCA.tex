\documentclass[10pt]{beamer}
\usepackage{bbm}

\usepackage{graphicx}
\usepackage{animate}
\renewcommand\appendixname{Appendix}
\usepackage{xcolor}
\usepackage{tikz}
\colorlet{rred}{red!80!black}
\colorlet{ggreen}{green!80!black}




\usetheme[progressbar=foot]{metropolis}
\usepackage{appendixnumberbeamer}
\setbeamercovered{dynamic}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setlength{\abovecaptionskip}{-10pt plus 0pt minus 0pt}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{obs}{Observation}

% math symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\YY}{\mathcal{Y}}
\newcommand{\UU}{\mathcal{U}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\WW}{\mathcal{W}}
\newcommand{\LL}{\mathcal{L}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\title{An introduction to PCA}
\subtitle{Weekly AI pills}
\date{2020-10-16}
\author{Fabio Brau.}
\institute{SSSA, Emerging Digital Technologies, Pisa.}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\usebackgroundtemplate{%
    \begin{picture}(300,271)
      \hspace{11.2cm}
       \includegraphics[scale=0.1]{pic/logoretis_noname.png}
   \end{picture}}

\begin{document}
{\usebackgroundtemplate{%
    \begin{picture}(300,265)
      \hspace{0.9cm}
       \includegraphics[scale=0.5]{pic/tecip_logo.png}
       \hspace{0.5cm}
       \includegraphics[scale=0.21]{pic/logoretis_320.png}
   \end{picture}}%
\maketitle
}
\begin{frame}{Summary}
  \begin{itemize}
    \item The aim of Principal Component Analysis
    \item Derivation
      \begin{enumerate}
          \item A Geometrical idea
          \item A statistical Derivation
          \item Singolar Value Decomposition
      \end{enumerate}
    \item PCA from Encoder Decoder NN
    \item Dummy examples
  \end{itemize}
\end{frame}
\section{Geometrical Introduction}
\begin{frame}{Geometrical Introduction}
  Let $X \in \R^{N\times n}$ be a dataset of $N$
  {\bf observation} within $n$ {\bf variables}. 
  \begin{equation}
    X =
    \begin{bmatrix}
      \, & x_1 ^ T &\,\\
      \, & \vdots &\,\\
      \, & x_N ^ T &\,\\
    \end{bmatrix}
    =
    \begin{bmatrix}
      & & & \\
      x^{(1)} &\vline& \cdots& \vline& x^{(n)}\\
      & & & \\
    \end{bmatrix}
    \label{Geometrical view}
  \end{equation}
  {\bf Notations:}
  \begin{itemize}
    \item $x_i\in\R^n$ represents a single {\bf observation}, i.e a {\bf
      sample} in the feature space.
    \item $x^{(i)}\in\R^N$ represents the single {\bf variable}, i.e a {\bf
      column} of the dataset.
    \item The object $\1_n\in\R^n$ is the unitary columnar vector of length
      $n$ $\1_n=[1,\cdots,1]^T$.
    \item $X$ is centered if $X^T\1=0$
  \end{itemize}
\end{frame}
\begin{frame}{Geometrical Introduction: Finding a principal direction.}
  \begin{minipage}{0.5\textwidth}
    \begin{enumerate}
      \item \uncover<1->{Scalar product measures the projection of $x_j$ along
        the direction $w$.}
      \item \uncover<2->{We are only interested on module.}
      \item \uncover<3->{Summation over samples to get the global
        projection's contribute.}
      \item \uncover<4->{Searching for $w$ which maximizes projection.}
      \item \uncover<5->{Adding constraint to avoid $w\to\infty$ solution.}
    \end{enumerate}
  \end{minipage}\hfill
  \begin{minipage}{0.4\textwidth}
      \begin{equation*}
        \uncover<4->{w_1 \in\argmax_{\uncover<5>{\|w\|_2 = 1}}}
        \uncover<3->{\sum_{j=1}^N}
        \uncover<2->{\left(}\uncover<1->{w\cdot x_j}\uncover<2->{\right)^2}
      \end{equation*}
    \begin{figure}[b]
      \centering
      \includegraphics<1-2>[clip, scale=0.3, trim=3.1cm 0 0cm 0cm]{./pic/dummy_2.eps}
      \includegraphics<3->[clip, scale=0.3, trim=3.1cm 0 0cm 0cm]{./pic/dummy_3.eps}
    \end{figure}
  \end{minipage}
\end{frame}
\begin{frame}{Geometrical Introduction: Finding other directions}
  We search for other orthogonal directions which maximize projections.
  \begin{equation*}
  \begin{aligned}
    w_1 \in \argmax_{\|w\|_2=1}\sum_{j=1}^N (w\cdot x)^2 &\\
    w_2 \in \argmax_{\|w\|_2=1}\sum_{j=1}^N (w\cdot x)^2 &\quad\mbox{and}\quad w_2\perp w_1\\
    \vdots \\
    w_n \in \argmax_{\|w\|_2=1}\sum_{j=1}^N (w\cdot x)^2 &\quad\mbox{and}\quad w_2\perp \left\{
    w_1,\ldots,w_{n-1} \right\}
  \end{aligned}
\end{equation*}
\begin{center}
  \href{./pic/PCA_3D.html}{\bf Example}
\end{center}
\end{frame}
\metroset{block=fill}
\begin{frame}{Geometrical Introduction: Direction Selection}
  \[
    V(w)=\sum_j (w \cdot x_j)^2 \qquad\mbox{\bf momentum along $w$}
  \]
  \onslide<2->
  If $w_1,\,w_2,\,w_3$ orthogonal that maximizes $V$ in the 3D example, then
  \begin{enumerate}
    \item $V(w_1) =  3181.20$\hfill\uncover<3->{\approx82.5\%}
    \item $V(w_2) = 646.25$\hfill\uncover<3->{\approx 17.0\%}
    \item $V(w_3) = 19.23$ \hfill\uncover<3->{\approx 0.5 \%}
  \end{enumerate}
    \onslide<4->
    \begin{center}
      {\bf What if we forget the last direction?}
    \end{center}
  \onslide<5->
  \begin{block}{Observation}
    \begin{itemize}
        \item $x_j =\alpha_{1j}w_1+\alpha_{2j}w_2+\alpha_{3j}w_3$
          (where $\alpha_{ij} = w_i\cdot x_j$).
        \item $\tilde x_j = \alpha_{1j}w_1 + \alpha_{2j}w_2$. 
    \end{itemize}
    \begin{equation}
      \frac{1}{N}\sum_j \|x_j - \tilde x_j\|^2 = \frac{V(w_3)}{N}\approx
      4.8\,10^{-3}
      \tag{MSE}
    \end{equation}
  \end{block}
\end{frame}
\begin{frame}{Geometrical Introduction: Conclusion}
  \begin{itemize}
    \item Given a set of data $X\in\R^{N\times n}$
    \item We can find $w_1,\cdots,w_n$ principal (orthonormal) directions the
      maximize their momentum.
    \item $V(w_1) > V(w_2)>\cdots> V(w_n)$
    \item Approximating X with $\tilde X$ by taking only the first $k$ directions we are
    getting an error that is $V(w_{k+1})/N$
  \end{itemize}
  \onslide<2->
  {\bf\centering What's the catch?}
  \onslide<3->
   \begin{equation}
     \begin{aligned}
       \max_{w\in\R^n}\quad & \sum_{j=1}^N (w \cdot x_j)^2\\
       \mbox{s.t}\quad & w_i \cdot w =0, \,\forall i<k \\
       & w\cdot w = 1\\
    \end{aligned}
    \label{MP}
    \tag{MP}
  \end{equation} 
\end{frame}
\section{Classical Derivation}
\begin{frame}{Classical Derivation: Notations}{}
$\VV$ random variable, $V = (v_1,\ldots,v_N)$ N observations of the variable. 
  \begin{itemize}
    \item\uncover<2->{{\bf Expected Value} \hfill $\E[\VV] =
      \frac{1}{N}\sum_{j=1}^N v_j$} 
    \item\uncover<3->{{\bf Variance} \hfill $Var(\VV) = \E[(\VV -
      \E[\VV])^2]$}
    \item\uncover<4->{{\bf Covariance} \hfill $Cov(\UU,\VV) = \E[(\UU -
      \E[\UU])(\VV - \E[\VV])]$}
  \end{itemize}
  \onslide<5->

  {\bf Observations}\\
  Under the assumption $\E[\UU]=\E[\VV]=0$
  \begin{enumerate}
    \item $Var(\VV)=\frac{1}{N}\sum_{j=1}^N v_j^2$
    \item $Cov(\UU,\VV) = \frac{1}{N}\sum_{j=1}^N u_jv_j$ 
  \end{enumerate}
\end{frame}
\begin{frame}{Classical Derivation: An Eigenvalue Problem}
  \[
    \max_{\|w\|=1} V(w) =\max_{\|w\|=1} \sum_j (w^Tx_j)^2 = \max_{w^Tw=1}
    w^T(X^TX)w  
    \tag{MP}
  \]
  \onslide<2->
  \begin{center}
    {\bf Lagrange Multipliers Technique}
  \end{center}
  Let consider the Lagrangian Function of \ref{MP}
  \[
    \LL(w,\lambda) = V(w) - \lambda(w^Tw -1),\quad\forall w
    \in\R^n,\,\lambda\in\R
  \]
  \begin{block}{Claim}
  If $w^*$ is a solution of \ref{MP} then there exists $\lambda^*$ such that
  \begin{equation}
    \nabla \LL(w^*,\lambda^*)=0,\quad i.e\quad
      (X^TX) w^* -\lambda^*w^*=0
  \end{equation}
  \end{block}
  \onslide<3->
  \[
    w\, \mbox{\bf Principal Direction}\quad\Longrightarrow\quad w\,
    \mbox{\bf Eigenvector of}\, X^TX
  \]
\end{frame}
\begin{frame}{Classical Derivation: An Eigenvalue Problem}
  \begin{center}
    {\bf Why switching to an eigen-pair problem?}
  \end{center}
  \onslide<2->
  \begin{minipage}[t]{0.5\textwidth}
    \begin{minipage}[t]{0.3\textwidth}
      \vfill
      $X^TX\qquad +\qquad$
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
      \begin{figure}[h!]
        \centering
        \includegraphics<2->[scale=0.10, trim=0cm 0cm 0 3cm]{./pic/matlab.png}
      \end{figure}
    \end{minipage}
    \begin{minipage}[t]{0.1\textwidth}
    \vfill
    $\qquad\longrightarrow$
    \end{minipage}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.5\textwidth}
    \vspace{-0.4cm}
    \begin{itemize}
      \item $w_1,\cdots,w_n$ \hfill {\bf eigenvectors}
      \item $w_i^TX^TXw_i=V(w_i)$ \hfill{\bf eigenvalues}
      \item $V(w_1)>\cdots>V(w_n)$
    \end{itemize}
  \end{minipage}
\end{frame}
\begin{frame}{Classical Derivation: Dimensionality Reduction}
  The matrix $W=[w_1|\cdots|w_n]$ can be used to reduce the dimensionality
  \[
    F=
    \begin{bmatrix}
      f^{(1)}&|\cdots|&f^{(n)}
    \end{bmatrix}
    = X\,W\quad\mbox{\bf (factors scores)}
  \]
  \begin{minipage}[t]{0.4\textwidth}
    \begin{figure}[h!]
      \centering
      \includegraphics[clip, scale=0.4, trim=1cm 1.5cm 8cm 4cm]{./pic/PCA_3D.png}
      \caption{Feature space}
    \end{figure}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.5\textwidth}
    \begin{figure}[h!]
      \centering
      \includegraphics[scale=0.3]{./pic/PCA_2D.png}
      \caption{Factor scores restricted to the first two principal
      directions.}
    \end{figure}
  \end{minipage}
\end{frame}
\begin{frame}{Classical Derivation: Dimensionality Reduction}
  \animategraphics[loop,controls,width=0.5\linewidth]{2}{./pic/mnist/compress-}{0}{15}
  \begin{block}{Summary}{}
    \[
      \left\|
      \begin{bmatrix}
        f^{(1)} &|\cdots|& f^{(n-k)}
      \end{bmatrix}
      \begin{bmatrix}
        w_1^T\\
        \vdots\\
        w_{n-k}
      \end{bmatrix} - X\right\|<=V(w_k)+\cdots+V(w_n)
    \]
  \end{block}
\end{frame}
\end{document}

